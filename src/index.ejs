<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <!--TODO: Find a catchier title-->
    <h1 id="article-title">Curriculum Induction for Safe Reinforcement Learning</h1>
    <!--<p>
      Safe RL<d-footnote id="d-footnote-1">Reinforcement Learning</d-footnote> using curriculum learning by a trained teacher.
      A NI-Project<d-footnote id="d-footnote-1">Neural Information Processing Project</d-footnote>
      at TU Berlin<d-footnote id="d-footnote-1"><a href="https://www.tu.berlin/">Technical University of Berlin</a></d-footnote>
      based on <d-cite key="Turchetta2020SafeRL"></d-cite>.
    </p>-->
  </d-title>

  <d-article id="main-article">
    <d-contents id="toc">
			<nav class="toc figcaption" id="menu">
        <h4>Contents</h4>
        <div><a href="./#article-title">Introduction</a></div>
        <div><a href="./#methodology">Methodology</a></div>
        <div><a href="./#experiments">Experiments</a></div>
        <div><a href="./#results">Results</a></div>
        <div><a href="./#conclusion">Conclusion</a></div>
        <div><a href="./#outlook">Outlook</a></div>
			</nav>
			<div class="toc-line"></div>
		</d-contents>

    <h4>The Idea</h4>

    <p>
      The key idea of <i>Curriculum Induction for Safe RL</i> (CISR) <d-cite key="Turchetta2020SafeRL"></d-cite> is that a teacher safely trains a student to solve a given task.
      As this keeps the RL agent already safe during training, this method can be helpful in safety-critical systems.
    </p>
    <p>
      The teacher needs a set of interventions, i.e. it it needs to know how to detect dangerous states and how to save the student, but it doesn't need to know how to solve the task itself.
      In which order and how long each intervention is applied, i.e. the curriculum policy, is learned by the teacher by training multiple students.
    </p>

    <h4>Our Project</h4>
    <p>
      We compare the students trained by the Optimized curriculum policy from the paper <d-cite key="Turchetta2020SafeRL"/></d-cite>
      to students trained with our own curriculum policies and <!--TODO: more-->.
    </p>

    <h2 id="methodology">Methodology</h2>

    <p>
      In CISR, the student is a RL agent trained in a Constrained Markov Decision Process (CMDP), which is created by the teacher in each interaction unit using an intervention $i$ as described below.
    </p>
    <p style="text-align:center">
      $\mathcal{M}_i = \langle \mathcal{S},\mathcal{A},\mathcal{P}_i,r_i,\mathcal{D}, \mathcal{D}_i \rangle$
    </p>
    <ul>
      <li>$\mathcal{S}, \mathcal{A}$: State and action space</li>
      <li>$\mathcal{P}_i(s'|s,a)$: Transition kernel mapping pairs of states $s\in \mathcal{S}$ and actions $a\in A$ to new states $s'\in \mathcal{S}$</li>
      <li>$r_i:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$: Reward function</li>
      <li>$\mathcal{D}$: Set of unsafe terminal states</li>
      <li>$\mathcal{D}_i$: Set of trigger states</li>
    </ul>

    <p>
      The teacher gets a set $\mathcal{I}$ of interventions $\{ \langle \mathcal{D}_i, \mathcal{T}_i \rangle \}_{i=1}^K$ as input, which consist of trigger states $\mathcal{D}_i \subset \mathcal{S}$ and reset distributions $\mathcal{T}_i: \mathcal{S} \rightarrow \Delta_{\mathcal{S} \backslash \mathcal{D}_i}$.
    </p>
    <p>
      At he beginning of each interaction unit $n\in [N_s]$, the teacher decides on an intervention $i_n \in \mathcal{I}$, which induces the CMDP $M_{i_n}$ described above. This decision is done using the teachers curriculum policy $\pi^T: \mathcal{H} \rightarrow \mathcal{I}$, which maps the teachers observation history $\phi(\pi_1),...,\phi(\pi_{n-1})\in\mathcal{H}$ to the intervention $i_n$. As it is learned, we call this the optimized curriculum policy from now on.
    </p>
    <p>
      Curriculum policies independent of the student's policy (e.g. SR, HR, Back or Incremental) are simply a mapping $\pi^T:[N_s]\rightarrow \mathcal{I}$, assigning each interaction unit a specific intervention.
    </p>
    <p>
      The sequence of CMDPs $\mathcal{M}_{i_1},...,\mathcal{M}_{i_{N_s}}$, induced from the curriculum policy, is called curriculum.
    </p>
    <p>
      If the student enters a trigger state $s\in \mathcal{D}_i$, the transition is modified such that $\mathcal{P}_i(s'|s,a) = \mathcal{T}_i(s'|s)$. Additionally, the reward is cleared, i.e. $r_i(s,a,s')=0$. To prevent the student from relying on interventions, a constraint on the number of times the teachers can help the student is set. It is enforced by the CMDP solver, which penalizes the student for excessive use of the teachers help.
    </p>

    <figure>
      <div style="max-width: 25em; margin: 0 auto;">
        <%= require("../static/images/optimized_switch.svg") %>
      </div>
      <figcaption>The Optimized curriculum policy switching interventions from Soft Reset 1 (SR1 moves the agent one step back) to Hard Reset (HR resets the agent back to the start).</figcaption>
    </figure>

    <h2 id="experiments">Experiments</h2>

    <h4>Curriculum Policies</h4>

    <h5>The Back Policy</h5>
    <p>
      One of the simplest curriculum policy one could think of involves always going back $x$ steps when a trigger state is visited.
      In our experiments, we tested values for $x$ in the interval $[1,9]$.
    </p>

    <d-code block="" language="python">
      class Back(object):
      """
      Teacher that goes back a constant number of steps
      """
      def __init__(self, action_sequence, x=None):
          self.actions = action_sequence
          self.x = x

      def predict(self, obs):
          return self.actions[self.x - 1], None
    </d-code>

    <d-figure id="back-visualization">
      <figure>
        <div id="back-visualization-target"></div>
        <figcaption>
          Successes for the Optimized curriculum policy and the Back policy for different values of $x$.
        </figcaption>
      </figure>
    </d-figure>

    <h5>The Incremental Policy</h5>

    <p>
      The idea behind the Incremental curriculum policy is the tradeoff between exploration and exploitation.
      While the agent should be free to explore the map in the beginning, it should be punished harder for failures as the learning process progresses.
      This is realized by incrementally increasing the amount of steps the agent is reset in a linear fashion.
      Formally, Incremental$_x$ resets the agent by $\lceil \frac{1}{2^x} \cdot n \rceil$ steps in the $n^{\textrm{th}}$ curriculum step.
      During our experiments, we tried out values for $x$ in the range $[0,4]$.
    </p>

    <d-code block="" language="python">
      class IncrementalTeacher(object):
      """
      Incremental heuristic teacher that increases the buffer
      size on each curriculum step
      """
      def __init__(self, action_sequence, x=None):
          self.actions = action_sequence
          self.step = 0
          self.x = x

      def predict(self, obs):
          action = int(np.ceil((1/(2 ** self.x)) * self.step))
          self.step += 1
          return self.actions[action], None
    </d-code>

    <d-figure id="incremental-visualization">
      <figure>
        <div id="incremental-visualization-target"></div>
        <figcaption>
          Successes for the Optimized curriculum policy and the Incremental policy for different values of $x$.
        </figcaption>
      </figure>
    </d-figure>

    <h4>Environments</h4>

    <p>
      We used the two environments which are shown below. Both are based on the Frozen Lake environment from the gym-library <d-cite key="gym-flake"></d-cite>.
      The idea of the environments is, that the agent should find its way over a frozen lake, from start to goal, while avoiding holes in the ice.
      The trigger states, which the teacher uses to detect when invterventions should be done, are positioned around the dangerous holes.
      The environments are implemented as two-dimensional square grid-worlds, where the agent can move up/down/left/right.
    </p>

    <figure>
      <div style="max-width: 25em; margin: 0 auto;">
        <%= require("../static/images/maps.svg") %>
      </div>
      <figcaption>The Frozen Lake environment used in the paper<d-cite key="Turchetta2020SafeRL"></d-cite> on the left (size 10x10) and our Frozen Smiley environment on the right (size 16x16). Interventions are triggered at distance = 1 from holes.</figcaption>
    </figure>

    <h2 id="results">Results</h2>

    <h4>Student Performance</h4>

    <p>
      Adding the teacher with its trigger states and intervention transitions to the maps kept the students safe during training.
      When choosing the best parameters out of those we tested, the Back and Incremental policy were able to outperform the Optimized one.
      As the Frozen Smiley environment is slightly larger than the original one, the increasing path lenghts made an increase in reset steps for the Back policy necessary.
      Similarly, the Incremental curriculum policy took advantage of a longer exploration phase in the larger environment, by slowing down the increase of reset steps.
    </p>

    <figure>
      <div style="max-width: 25em; margin: 0 auto;">
        <%= require("../static/images/small_successes.svg") %>
      </div>
      <figcaption>Success rates of different curriculum policies on the Frozen Lake environment. For our policies, the best found parameters $x$ are used.</figcaption>
    </figure>

    <figure>
      <table style="width: 100%" id="table">
        <thead>
          <tr>
            <th></th>
            <th scope="col">Successes</th>
            <th scope="col">Training Failures</th>
            <th scope="col">Average Returns</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <th scope="row">Back$_6$</th>
            <td class="no">$0.921$</td>
            <td>$0.000$</td>
            <td class="no">$5.259$</td>
          </tr>
          <tr>
            <th scope="row">Incremental$_2$</th>
            <td>$0.886$</td>
            <td>$0.000$</td>
            <td>$5.053$</td>
          </tr>
          <tr>
            <th scope="row">Trained</th>
            <td>$0.743$</td>
            <td>$0.000$</td>
            <td>$4.035$</td>
          </tr>
          <tr>
            <th scope="row">Original</th>
            <td>$0.641$</td>
            <td class="no">$3672.600$</td>
            <td>$3.586$</td>
          </tr>
          <tr>
            <th scope="row">HR</th>
            <td>$0.000$</td>
            <td>$0.000$</td>
            <td>$-0.295$</td>
          </tr>
          <tr>
            <th scope="row">SR1</th>
            <td>$0.820$</td>
            <td>$0.000$</td>
            <td>$4.661$</td>
          </tr>
          <tr>
            <th scope="row">Bandit</th>
            <td>$0.659$</td>
            <td>$0.000$</td>
            <td>$3.638$</td>
          </tr>
        </tbody>
      </table>  
      <figcaption>
        Success rates, training failures and average returns of different curriculum policies on the Frozen Lake environment.
        All teachers keep the student safe while training the student in the original environment leads to failures during training.
        On average, the Back and Incremental policy outperform the others. 
      </figcaption>
    </figure>
    
    <figure>
      <div style="max-width: 25em; margin: 0 auto;">
        <%= require("../static/images/16x16_successes.svg") %>
      </div>
      <figcaption>
        Success rates of different curriculum policies on the Frozen Smiley environment. For our policies, the best found parameters $x$ are used.
      </figcaption>
    </figure>

    <figure>
      <table style="width: 100%" id="table">
        <thead>
          <tr>
            <th></th>
            <th scope="col">Successes</th>
            <th scope="col">Training Failures</th>
            <th scope="col">Average Returns</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <th scope="row">Back$_8$</th>
            <td class="no">$0.929$</td>
            <td>$0.000$</td>
            <td class="no">$5.293$</td>
          </tr>
          <tr>
            <th scope="row">Incremental$_4$</th>
            <td>$0.886$</td>
            <td>$0.000$</td>
            <td>$5.054$</td>
          </tr>
          <tr>
            <th scope="row">Trained</th>
            <td>$0.879$</td>
            <td>$0.000$</td>
            <td>$4.856$</td>
          </tr>
          <tr>
            <th scope="row">Original</th>
            <td>$0.741$</td>
            <td class="no">$2453.400$</td>
            <td>$4.166$</td>
          </tr>
          <tr>
            <th scope="row">HR</th>
            <td>$0.000$</td>
            <td>$0.000$</td>
            <td>$-0.140$</td>
          </tr>
          <tr>
            <th scope="row">SR1</th>
            <td>$0.597$</td>
            <td>$0.000$</td>
            <td>$3.367$</td>
          </tr>
          <tr>
            <th scope="row">Bandit</th>
            <td>$0.398$</td>
            <td>$0.000$</td>
            <td>$2.133$</td>
          </tr>
        </tbody>
      </table>  
      <figcaption>
        Success rates, training failures and average returns of different curriculum policies on the Frozen Smiley environment.
        All teachers keep the student safe while training the student in the original environment leads to failures during training.
        On average, the Back and Incremental policy outperform the others. 
      </figcaption>
    </figure>

    <h4>Trajectories</h4>
    <d-figure id="trajectories-visualization">
      <figure>
        <div id="trajectories-visualization-target"></div>
        <figcaption>
          Exemplary trajectories for the Frozen Smiley environment with the Optimized policy. 
          The lines represent the steps taken, while the background shows a heatmap of the student's positions.
          The trajectories show a progression from the first curriculum step (left) to a later step (right).
          <!--TODO: maybe use different trajectories and optimize images-->
        </figcaption>
      </figure>
    </d-figure>

    <h2 id="conclusion">Conclusion</h2>
    
    <p>
      During our experiments, we found that it is possible to define simple curriculum policies, which outperform a learned policy with little effort.
      We also observed that larger environments require a longer exploration phase and that the original HR, SR and Bandit policies do not generalize well to larger environments.
      Fundamentally, we found that defining reset transitions which keep the student safe is easier than defining suitable trigger states in the first place.
      When state spaces become more complex, dynamic or just partly observable, this can become a problem.
    </p>

    <h2 id="outlook">Outlook</h2>
    
    <p>
      Going forward, the method could be applied to OpenAI's Safety Gym envrioments to test the robustness of different strategies.
      Additionally, the amount of available interventions for the Optimized curriculum policy could be increased to account for this complexity.
      Finally, it has to be evaluated how well our curriculum policies generalize to dynamic or random envrionments.
    </p>

  </d-article>

  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      We are grateful to Rong Guo for supervising this project and continuously giving us feedback.
    </p>

    <h3>Author Contributions</h3>
    <p>
      This article was co-authored by Marvin Sextro and Jonas Loos under supervision of Rong Guo. Klaus Obermayer provided feedback on the project.
    </p>
    <p>† equal contributions</p>

    <h3>Additional Material</h3>
    <p>
      For a summary of the main findings presented in this article, also see our <a href="https://raw.githubusercontent.com/Safe-RL-Team/curriculum-learning-poster/main/curriculum_learning_poster.pdf">conference poster</a>.
    </p>

    <h3>Updates and Corrections</h3>
    <p>
      If you see mistakes or want to suggest changes, please <a href="https://github.com/Safe-RL-Team/curriculum-learning/issues/new">create an issue on GitHub</a>.
    </p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>