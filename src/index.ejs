<!doctype html>

<head>
  <title>Watch Your Step! - Safe Training in Reinforcement Learning</title>
  <meta charset="utf-8">
  <meta name="description" content="An Interactive Introduction to Curriculum Induction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="Safe, Reinforcement, Learning, Curriculum, Induction">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="template/template.v2.js"></script>
  <link rel="icon" href="favicon.ico" type="image/x-icon"/>
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon"/>  
  <link rel="apple-touch-icon" href="images/robot.png">
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1 id="article-title">Watch Your Step! - Safe Training in Reinforcement Learning</h1>
    <p>
      An Interactive Introduction to Curriculum Induction
    </p>
  </d-title>

  <d-byline></d-byline>

  <d-article id="main-article">

    <d-figure id="minigame" class="l-page-outset">
      <div id="minigame-target"></div>
    </d-figure>
    
    <d-abstract class="l-page-outset">
      <p>
        The minigame above lets you play the role of a Reinforcement Learning (RL) agent trying to maximize the reward by reaching a goal.
        Try to avoid the holes and navigate over the surface of the Frozen Lake with the arrow keys, but watch your step as the ice may be slippery.
        Use the drop-down menus to configure a teacher which helps you to avoid danger while solving the task.
      </p>
      <p>
        <i><b>Curriculum Learning</b> is all about applying the safeguard mechanisms you just tried out in an optimal way.</i>
      </p>
    </d-abstract>

    <div class="l-screen horizontal-line"></div>

    <d-contents id="toc">
      <div class="toc-wrapper">
        <nav class="figcaption" id="menu">
          <h4>Contents</h4>
          <div><a href="./#article-title">Introduction</a></div>
          <div><a href="./#background">Background</a></div>
          <div><a href="./#methodology">Methodology</a></div>
          <div><a href="./#experiments">Experiments</a></div>
          <div><a href="./#results">Results</a></div>
          <div><a href="./#limitations">Limitations</a></div>
          <div><a href="./#conclusion">Conclusion</a></div>
          <div><a href="./#outlook">Outlook</a></div>
        </nav>
        <div class="toc-line"></div>
      </div>
    </d-contents>

    <div id="abstract-text">
      <h4>The Idea</h4>

      <p>
        The key idea of <i>Curriculum Induction for Safe RL</i> (CISR) <d-cite key="Turchetta2020SafeRL"></d-cite> is that a teacher trains a student to solve a given task while avoiding failure.
        This can be helpful in safety-critical systems, as the agent is already being kept safe during training, preventing costly failures.
      </p>
      <p>
        To be able to save the student with a given set of interventions, the teacher needs to know how to detect dangerous states, but it does not need to know how to solve the task itself.
        The <i>curriculum policy</i> defines the order and duration in which interventions are applied.
        Learning the curriculum policy requires the teacher to train multiple students while assessing their performance.
      </p>

      <h4>Our Project</h4>
      <p>
        We give an interactive introduction to curriculum learning and provide the theoretical background to understand and apply the method.
        In our experiments, we compare the students trained by the Optimized curriculum policy proposed by Turchetta et al. <d-cite key="Turchetta2020SafeRL"></d-cite> to students trained with our own curriculum policies.
      </p>
    </div>


    <h2 id="background">Background</h2>

    <p>
      For the application of RL, safety can be the deciding factor in enabling or preventing the usage of a system.
      This is especially true for physical systems, as they can degrade or destroy themselves or even their environment.
      Thereby, it is not only important for the system to be safe after deployment, but also during training in the real world.
      Approaches to RL safety include Constrained Markov Decision Processes (CDMPs), as used in this work, budgeted MDPs, and Lyapunov functions.
      <d-cite key="DulacArnold2019ChallengesOR"></d-cite>
    </p>

    <p>
      An example application for the need of safety during training of RL systems are autonomous cars.
      While simulations are helpful and maybe a good starting point, training on the streets is still necessary.
      During this process, it is crucial to prevent crashes and harm to people, property and the car itself.
      <d-cite key="Turchetta2020SafeRL"></d-cite>
    </p>

    <p>
      CISR, as a form of curriculum learning, relies on a teacher as an aid during training of the agent, which is analogously called the student.
      To be able to help the student, the teacher is given a set of interventions and has to decide on when to apply which.
      For example, when teaching a child how to ride a bike, possible interventions may be adding training wheels, catching them when they fall or giving them knee and elbow guards.
      The order in which the interventions are applied has to be optimized as it can significantly impact the student's performance.
      In our analogy, it may be detrimental to skip the training wheels or to never remove them, since this would either give them no chance at getting started to learn the task or improving themselves.
      By training multiple students, the teacher is able to learn an optimal curriculum policy.
      <d-cite key="Turchetta2020SafeRL"></d-cite>
    </p>

    <p>
      In contrast to learning from demonstration <d-cite key="Argall2009ASO"></d-cite>, curriculum learning does not expect the teacher to know how to solve the task,
      but rather relies on the teacher to supervise and structure the learning process.
      A partly similar approach to CISR was introduced by Graves et al. <d-cite key="Graves2017AutomatedCL"></d-cite>, using a nonstationary multi-armed bandit algorithm to determine an optimized curriculum.
      Matiisen et al. <d-cite key="Matiisen2017TSCL"></d-cite> formalized the concept of learning a curriculum with an additional RL agent as <i>Teacher-Student Curriculum Learning</i> (TSCL) and applied the method to solve mazes in Minecraft.
    </p>

    <p>
      CISR can also be viewed as a meta-learning framework <d-cite key="Vanschoren2018MetaLearningAS"></d-cite>, optimizing the curriculum policy as a hyperparameter.
      In practice, the curriculum policy could be optimized in simulation or in simplified settings before being deployed for the actual training, where training time is scarce. 
      For example, this could make the training with physical robots faster and more secure.
      <d-cite key="Turchetta2020SafeRL"></d-cite>
    </p>


    <h2 id="methodology">Methodology</h2>

    <p>
      In CISR <d-cite key="Turchetta2020SafeRL"></d-cite>, the student is a RL agent trained in a Constrained Markov Decision Process (CMDP), which is created by the teacher in each interaction unit using an intervention $i\in\mathcal{I}$ as described below.
    </p>
    <p style="text-align:center">
      $\mathcal{M}_i = \langle \mathcal{S},\mathcal{A},\mathcal{P}_i,r_i,\mathcal{D}, \mathcal{D}_i \rangle$
    </p>
    <ul>
      <li>$\mathcal{S}, \mathcal{A}$: State and action space</li>
      <li>$\mathcal{P}_i(s'|s,a)$: Transition kernel mapping pairs of states $s\in \mathcal{S}$ and actions $a\in A$ to new states $s'\in \mathcal{S}$</li>
      <li>$r_i:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$: Reward function</li>
      <li>$\mathcal{D}$: Set of unsafe terminal states</li>
      <li>$\mathcal{D}_i$: Set of trigger states</li>
    </ul>

    <p>
      The teacher gets a set $\mathcal{I}$ of interventions $\{ \langle \mathcal{D}_i, \mathcal{T}_i \rangle \}_{i=1}^K$ as input, which consist of trigger states $\mathcal{D}_i \subset \mathcal{S}$ and reset distributions $\mathcal{T}_i: \mathcal{S} \rightarrow \Delta_{\mathcal{S} \backslash \mathcal{D}_i}$.
      If the student enters a trigger state $s\in \mathcal{D}_i$, the transition is modified such that $\mathcal{P}_i(s'|s,a) = \mathcal{T}_i(s'|s)$, leading the student to a safe state $s'\not\in \mathcal{D_i}$.
      An intervention by the teacher does not reduce the student's reward, i.e. $r_i(s,a,s')=0$ for $s\in \mathcal{D_i}$ and $s' \not\in \mathcal{D_i}$
      <d-footnote>
        To prevent the student from relying on interventions, a constraint on the number of times the teachers can help the student is set.
        It is enforced by the CMDP solver, which penalizes the student for excessive use of the teachers help.
      </d-footnote>.
    </p>
    <p>
      At he beginning of each interaction unit $n\in [N_s]$, the teacher decides on an intervention $i_n \in \mathcal{I}$, which induces a CMDP $\mathcal{M}_{i_n}$ as described above.
      This decision is done using the teachers curriculum policy $\pi^T: \mathcal{H} \rightarrow \mathcal{I}$, which maps the teacher's observation history $\phi(\pi_1),...,\phi(\pi_{n-1})\in\mathcal{H}$ to the intervention $i_n$.
      The observations $\phi(\pi_n)$ are <i>features</i> computed from the performance of the student, e.g. by taking an estimate of the the student's policy value $\hat{V}(\pi_n)$ or the number of necessary teacher interventions into account.
      As this curriculum policy is learned, we call it the Optimized curriculum policy from now on.
    </p>
    <p>
      Curriculum policies independent of the student's policy are simply a mapping $\pi^T:[N_s]\rightarrow \mathcal{I}$, assigning each interaction unit a specific intervention.
      Except for the Optimized curriculum policy, all policies we will work with in this article are of this kind.
      An advantage of student-independent curriculum policies is that they do not require a training process and therefore no measure of the student's performance.
    </p>
    <p>
      A sequence of CMDPs $\mathcal{M}_{i_1},...,\mathcal{M}_{i_{N_s}}$, induced from a curriculum policy, is called <i>curriculum</i>.
      The figure below shows a curriculum induced by the Optimized curriculum policy with two simple interventions, either resetting the agent to the start (Hard Reset) or moving them one step back (Soft Reset).
      Note that interaction units and curriculum steps mean the same and can be used interchangeably.
    </p>

    <d-figure>
      <figure>
        <div style="max-width: 25em; margin: 0 auto;">
          <%= require("../static/images/optimized_switch.svg") %>
        </div>
        <figcaption>
          The Optimized curriculum policy switching interventions from Soft Reset 1 (SR1 moves the agent one step back) to Hard Reset (HR resets the agent back to the start) after three interaction units.
        </figcaption>
      </figure> 
    </d-figure>

    <h4>Training</h4>

    <p>
      Below is the CISR algorithm, which shows how the curriculum policy is optimized.
      The teacher learns online in $N_t$ rounds and plays a <i>decision rule</i> $\pi^T_j$ that makes a new student $j$ learn under an adaptively constructed sequence of CMDPs $\mathcal{M}_{i_n}$.
      Each student $j$ learns via $N_s$ interaction units, updating its policy by transferring between units.
      Then, the teacher computes features $\phi$ by evaluating the student's policies, based on which it proposes the next CMDP.
      At the end of each round, the teacher adjusts its decision rule.
    </p>

    <d-figure>
      <figure>
        <div style="max-width: 30em; margin: 0 auto;">
          <%= require("../static/images/cisr.svg") %>
        </div>
        <figcaption>
          The CISR algorithm by Turchetta et al.<d-cite key="Turchetta2020SafeRL"></d-cite>.
        </figcaption>
      </figure>
    </d-figure>

    <div class="l-gutter figcaption">
      <ul style="list-style: none; padding: 0; margin:0;">
        <li>$N_t$: Number of rounds</li>
        <li>$N_s$: Interaction units</li>
        <li>$\pi_{n,j}$: Policy of the $j$th student in the $n$th interaction unit</li>
        <li>$\mathcal{M}_{i_n}$: CMDP induced by an intervention $i_n \in \mathcal{I}$</li>
        <li>$\pi^T_j$: Decision rule of the teacher for the $j$th student</li>
        <li>$o_n^T$: Teacher observations in the $n$th interaction unit</li>
        <li>$\phi(\pi_{n,j})$: Features of the $j$th student's policy in the $n$th interaction unit</li>
      </ul>
    </div>


    <h2 id="experiments">Experiments</h2>

    <p>
      Our experiments concentrated on comparing the Optimized policy to other student-independent curriculum policies and evaluating how well they generalize to an environment of different size.
      To accomplish this, we created our own Frozen Smiley environment, which is based on the Frozen Lake environment used by Turchetta et al. <d-cite key="Turchetta2020SafeRL"></d-cite>.
      In addition, we propose two new curriculum policies, which will be explained in detail in this section.
    </p>

    <h4>Environments</h4>

    <p>
      We used the two environments which are shown below. 
      Both are based on the Frozen Lake environment from the gym-library <d-cite key="gym-flake"></d-cite>.
      The idea of the environments is that the agent has to find its way over a frozen lake, from start to goal, while avoiding holes in the ice.
      They are implemented as two-dimensional square grid-worlds, where the agent can move in four directions.
      Moving to a safe state gives the agent a negative reward of $-0.01$, while reaching the goal rewards the agent with $6$ points.
      Interventions by the teacher do not impose costs on the agent, however failing resets the score for the round to $0$.
      Because ice is slippery, there is a $20\%$ chance that the agent moves to the side instead of forward, causing the game to be non-deterministic.
      The trigger states, which the teacher uses to detect when it has to intervene, are positioned around the dangerous holes within a pre-defined reachability distance.
      To get a better idea of how the environments behave, we recommend to try out the <a href="./#minigame">minigame</a> on top of this article.
    </p>

    <d-figure>
      <figure>
        <div style="max-width: 30em; margin: 0 auto;">
          <%= require("../static/images/maps.svg") %>
        </div>
        <figcaption>
          The Frozen Lake environment used by Turchetta et al. <d-cite key="Turchetta2020SafeRL"></d-cite> on the left (size $10 \times 10$) and our Frozen Smiley environment on the right (size $16 \times 16$).
          Interventions are triggered at distance $1$ from holes.
        </figcaption>
      </figure>
    </d-figure>

    <h4>Curriculum Policies</h4>

    <p>
      While a learned curriculum policy is presumably more generally applicable and does not require manual optimization, it also comes with the disadvantage of a training process and the need for a performance measure of the student.
      With this in mind, we came up with two simple curriculum policies of the form $\pi^T : [N_s] \rightarrow \mathcal{I}$, which will be described in the following.
    </p>

    <h5>The Back Policy</h5>
    <p>
      One of the simplest curriculum policy one could think of involves always going back $x$ steps when a trigger state is visited.
      For our experiments, we tested values for $x$ in the interval $[1,9]$.
      When playing the <a href="./#minigame">minigame</a> on top of this article, you can try out the Back$_4$ policy by yourself and see how it influences training.
      Below is our implementation in Python, which simply involves a class taking $x$ as an input and always taking the action at index $x-1$, which corresponds to going back $x$ steps.
    </p>

    <figure>
      <div>
        <d-code block="" language="python">
          class Back:
            """
            Teacher that goes back a constant number of steps
            """
            def __init__(self, action_sequence, x=None):
              self.actions = action_sequence
              self.x = x
      
            def predict(self, obs):
              return self.actions[self.x - 1], None
        </d-code>
      </div>
      <figcaption>
        Our implementation of the Back$_x$ curriculum policy. The $n$th element of the $\texttt{action\_sequence}$ list corresponds to the action which resets the agent by $n$ steps.
      </figcaption>
    </figure>

    <p>
      To compare the Back curriculum policy with the Optimized one, we plotted them for different values of $x$ below.
      Move the slider and see how they measure up in the Frozen Smiley environment for yourself.
    </p>

    <d-figure id="back-visualization">
      <figure>
        <div id="back-visualization-target"></div>
        <figcaption>
          Successes for the Optimized curriculum policy and the Back policy for different values of $x$ in the Frozen Smiley environment. 
          The transparent areas show the standard deviation for $N_t = 10$ students.
        </figcaption>
      </figure> 
    </d-figure>

    <h5>The Incremental Policy</h5>

    <p>
      The idea behind the Incremental curriculum policy is the tradeoff between exploration and exploitation.
      While the agent should be free to explore the map in the beginning, it should be punished harder for failures as the learning process progresses.
      This is realized by incrementally increasing the amount of steps the agent is reset linearly.
      Formally, Incremental$_x$ resets the agent by $\lceil \frac{1}{2^x} \cdot n \rceil$ steps in the $n$th curriculum step.
      During our experiments, we tried out values for $x$ in the range $[0,4]$.
      Our implementation below simply increases a counter after each intervention unit and scales it linearly with the parameter $x$.
      The rounded result is then used as the index for the action, corresponding to the number of steps the agent is reset by.
    </p>

    <figure>
      <div>
        <d-code block="" language="python">
          class IncrementalTeacher:
            """
            Incremental heuristic teacher that increases the buffer
            size on each curriculum step
            """
            def __init__(self, action_sequence, x=None):
              self.actions = action_sequence
              self.step = 0
              self.x = x
      
            def predict(self, obs):
              action = int(np.ceil((1/(2 ** self.x)) * self.step))
              self.step += 1
              return self.actions[action], None
        </d-code>
      </div>
      <figcaption>
        Our implementation of the Incremental$_x$ curriculum policy. The $n$th element of the $\texttt{action\_sequence}$ list corresponds to the action which resets the agent by $n$ steps.
      </figcaption>
    </figure>

    <p>
      Below we plotted the Incremental and the Optimized curriculum policy for different values of $x$.
      Again, use the slider to try out which value of $x$ works best in the Frozen Smiley environment.
    </p>

    <d-figure id="incremental-visualization">
      <figure>
        <div id="incremental-visualization-target"></div>
        <figcaption>
          Successes for the Optimized curriculum policy and the Incremental policy for different values of $x$ in the Frozen Smiley environment.
          The transparent areas show the standard deviation for $N_t = 10$ students.
        </figcaption>
      </figure>
    </d-figure>


    <h2 id="results">Results</h2>

    <p>
      In this section, we take a look at how our curriculum policies compare to the others and see how the larger Frozen Smiley environment influences their performance.
    </p>

    <h4>Student Performance</h4>

    <p>
      For our comparisons, we measure the successes, training failures and average rewards both for the Frozen Lake and the Frozen Smiley environment.
      Each curriculum policy was applied to $10$ students to account for errors and the randomness of the environment.
    </p>

    <h5>Frozen Lake</h5>

    <p>
      From experimentation with the values of $x$ like described in the previous section, we found that in the Frozen Lake environment $x=6$ and $x=2$ worked best for the Back and Incremental curriculum policies respectively.
      In this configuration, both the Back and the Incremental policy surpass the others within the first curriculum steps and quickly reach a success rate around $90\%$.
    </p>

    <d-figure>
      <figure>
        <div style="max-width: 25em; margin: 0 auto;">
          <%= require("../static/images/small_successes.svg") %>
        </div>
        <figcaption>
          Success rates of different curriculum policies on the Frozen Lake environment.
          For our policies, the best found parameters $x$ are used.
          The transparent areas show the standard deviation for $N_t = 10$ students.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      This is also reflected in the table below, which shows that all teachers keep the student safe while training the student in the original environment leads to failures during training.
      On average, the Back and Incremental policy outperform the others with the Back policy performing slightly better than the Incremental policy.
    </p>

    <figure>
      <div>
        <table style="width: 100%" id="table">
          <thead>
            <tr>
              <th></th>
              <th scope="col">Successes</th>
              <th scope="col">Training Failures</th>
              <th scope="col">Average Returns</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Back$_6$</th>
              <td class="no">$0.921$</td>
              <td>$0.000$</td>
              <td class="no">$5.259$</td>
            </tr>
            <tr>
              <th scope="row">Incremental$_2$</th>
              <td>$0.886$</td>
              <td>$0.000$</td>
              <td>$5.053$</td>
            </tr>
            <tr>
              <th scope="row">Trained</th>
              <td>$0.743$</td>
              <td>$0.000$</td>
              <td>$4.035$</td>
            </tr>
            <tr>
              <th scope="row">Original</th>
              <td>$0.641$</td>
              <td class="no">$3672.600$</td>
              <td>$3.586$</td>
            </tr>
            <tr>
              <th scope="row">HR</th>
              <td>$0.000$</td>
              <td>$0.000$</td>
              <td>$-0.295$</td>
            </tr>
            <tr>
              <th scope="row">SR1</th>
              <td>$0.820$</td>
              <td>$0.000$</td>
              <td>$4.661$</td>
            </tr>
            <tr>
              <th scope="row">Bandit</th>
              <td>$0.659$</td>
              <td>$0.000$</td>
              <td>$3.638$</td>
            </tr>
          </tbody>
        </table>
      </div>
      <figcaption>
        Success rates, training failures and average returns of different curriculum policies on the Frozen Lake environment.
        The highest values in each column are highlighted.
      </figcaption>
    </figure>

    <h5>Frozen Smiley</h5>

    <p>
      For the Frozen Smiley environment, we found that $x=8$ works best for the Back curriculum policy, while Incremental performs best for $x=4$.
      Noticeably, the increase within the first curriculum steps is not as steep in this environment, only reaching a stable success rate around $90\%$ after $6$ intervention units.
      Even more apparent is the lower performance of the SR1 (Soft Reset 1) and the Bandit policy.
    </p>

    <d-figure>
      <figure>
        <div style="max-width: 25em; margin: 0 auto;">
          <%= require("../static/images/16x16_successes.svg") %>
        </div>
        <figcaption>
          Success rates of different curriculum policies on the Frozen Smiley environment.
          For our policies, the best found parameters $x$ are used.
          The transparent areas show the standard deviation for $N_t = 10$ students.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      Like previously, all teachers keep the students safe during training as seen below.
      The Optimized curriculum policy manages to keep up with the Incremental policy in terms of successes after $10$ interaction units.
      Still, the Back policy performs best at a final success rate of over $90\%$ and the largest average returns.
    </p>

    <figure>
      <div>
        <table style="width: 100%" id="table">
          <thead>
            <tr>
              <th></th>
              <th scope="col">Successes</th>
              <th scope="col">Training Failures</th>
              <th scope="col">Average Returns</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Back$_8$</th>
              <td class="no">$0.929$</td>
              <td>$0.000$</td>
              <td class="no">$5.293$</td>
            </tr>
            <tr>
              <th scope="row">Incremental$_4$</th>
              <td>$0.886$</td>
              <td>$0.000$</td>
              <td>$5.054$</td>
            </tr>
            <tr>
              <th scope="row">Trained</th>
              <td>$0.879$</td>
              <td>$0.000$</td>
              <td>$4.856$</td>
            </tr>
            <tr>
              <th scope="row">Original</th>
              <td>$0.741$</td>
              <td class="no">$2453.400$</td>
              <td>$4.166$</td>
            </tr>
            <tr>
              <th scope="row">HR</th>
              <td>$0.000$</td>
              <td>$0.000$</td>
              <td>$-0.140$</td>
            </tr>
            <tr>
              <th scope="row">SR1</th>
              <td>$0.597$</td>
              <td>$0.000$</td>
              <td>$3.367$</td>
            </tr>
            <tr>
              <th scope="row">Bandit</th>
              <td>$0.398$</td>
              <td>$0.000$</td>
              <td>$2.133$</td>
            </tr>
          </tbody>
        </table>
      </div>
      <figcaption>
        Success rates, training failures and average returns of different curriculum policies on the Frozen Smiley environment.
        The highest values in each column are highlighted.
      </figcaption>
    </figure>

    <h4>Trajectories</h4>

    <p>
      The figure below shows a visualization of the paths taken by the students during training.
      During the first curriculum steps we can see the students exploring the map, while at later steps, they follow the previously found path, with only slight deviations or optimizations.
    </p>

    <d-figure id="trajectories-visualization">
      <figure>
        <div id="trajectories-visualization-target"></div>
        <figcaption>
          Exemplary trajectories for the Frozen Smiley environment with the Optimized policy. 
          The lines represent the steps taken, while the background shows a heatmap of the student's positions.
          The trajectories show a progression from the first curriculum step to a later step.
        </figcaption>
      </figure>
    </d-figure>

    <h4>Evaluation</h4>

    <p>
      Adding the teacher with its trigger states and intervention transitions to the maps kept the students safe during training.
      When choosing the best parameters out of those we tested, the Back and Incremental policy were able to outperform the Optimized one.
      Both the higher values for $x$ and slower increase in success rates could be explained by the increased size of the environment.
      As the Frozen Smiley environment is slightly larger than the original one, the increasing path lengths made an increase in reset steps for the Back policy necessary.
      Similarly, the Incremental curriculum policy took advantage of a longer exploration phase in the larger environment, by slowing down the increase of reset steps.
    </p>


    <h2 id="limitations">Limitations</h2>

    <p>
      While the described method has several clear advantages, there are also disadvantages and limitations coming along with it.
      The predominant problem we faced during our experiments is the difficulty of defining suitable trigger states for the teacher, which can be a complex task in environments with continuous state spaces and observations.
      While the Frozen Lake environment allows to simply define which states trigger the teacher's interventions by using a distance measure from danger zones, this is less trivial for other environments.
      For example, the observation space of the car racing environment <d-cite key="gym-car-racing"></d-cite> consists of $96 \times 96$ pixel images.
      In this case, defining the trigger states would require to keep track of the car's position and to work with thresholds.
      In addition, the set of possible reset transitions has to be hand-crafted and the teacher needs to be trained, requiring additional computational resources.
    </p>

    <p>
      Some tasks might also be simply so risky, that the teacher prevents the student from solving them at all, because it prevents the student from taking necessary risks.
      An example for this can be the game at the beginning, if the Frozen Smiley environment and the teacher with trigger states within two tiles around danger zones are selected.
      In these cases, less restrictive trigger states are needed, which might have to allow failures during training.
    </p>

    <h2 id="conclusion">Conclusion</h2>
    
    <p>
      During our experiments, we found that it is possible to define simple curriculum policies, which outperform a learned policy with little effort.
      We also observed that larger environments require a longer exploration phase and that the original HR, SR and Bandit policies do not generalize well to larger environments.
      Fundamentally, we found that defining reset transitions which keep the student safe is easier than defining suitable trigger states in the first place.
      When state spaces become more complex, dynamic or just partly observable, this could become a problem.
    </p>


    <h2 id="outlook">Outlook</h2>

    <p>
      Going forward, the method could be applied to other environments in OpenAI's Safety Gym <d-cite key="Ray2016SafetyGym"></d-cite> to test the robustness of different strategies.
      Additionally, the amount of available interventions for the Optimized curriculum policy could be increased to account for this complexity.
      Finally, it has to be evaluated how well our curriculum policies generalize to dynamic or random environments.
    </p>

  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      We are grateful to Rong Guo for supervising this project and continuously giving us feedback.
    </p>

    <h3>Author Contributions</h3>
    <p>
      This article was co-authored by Marvin Sextro and Jonas Loos under supervision of Rong Guo. Klaus Obermayer provided feedback on the project.
    </p>
    <p>* equal contributions</p>

    <h3>Additional Material</h3>
    <p>
      For a summary of the main findings presented in this article, also see our <a href="https://raw.githubusercontent.com/Safe-RL-Team/curriculum-learning-poster/main/curriculum_learning_poster.pdf">conference poster</a>.
    </p>

    <h3>Updates and Corrections</h3>
    <p>
      If you see mistakes or want to suggest changes, please <a href="https://github.com/Safe-RL-Team/curriculum-learning/issues/new">create an issue on GitHub</a>.
    </p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>