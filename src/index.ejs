<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="template/template.v2.js"></script>
  <link rel="icon" href="favicon.ico" type="image/x-icon"/>
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon"/>  
  <link rel="apple-touch-icon" href="images/robot.png">
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1 id="article-title">Watch Your Step! - Safe Training in Reinforcement Learning</h1>
    <p>
      An Interactive Introduction to Curriculum Induction
    </p>
  </d-title>

  <d-byline></d-byline>

  <d-article id="main-article">

    <d-figure id="grid-visualization" class="l-page-outset">
      <div id="grid-visualization-target"></div>
    </d-figure>
    
    <d-abstract class="l-page-outset">
      <p>
        The minigame above lets you play the role of a Reinforcement Learning (RL) agent trying to maximize the reward by reaching a goal.
        Try to avoid the holes and navigate over the surface of the Frozen Lake with the arrow keys, but watch your step as the ice may be slippery.
        Use the drop-down menus to configure a teacher which helps you to avoid danger while solving the task.
      </p>
      <p>
        <i><b>Curriculum Learning</b> is all about applying the safeguard mechanisms you just tried out in an optimal way.</i>
      </p>
    </d-abstract>

    <div class="l-screen horizontal-line"></div>

    <d-contents id="toc">
      <div class="toc-wrapper">
        <nav class="figcaption" id="menu">
          <h4>Contents</h4>
          <div><a href="./#article-title">Introduction</a></div>
          <div><a href="./#background">Background</a></div>
          <div><a href="./#methodology">Methodology</a></div>
          <div><a href="./#experiments">Experiments</a></div>
          <div><a href="./#results">Results</a></div>
          <div><a href="./#limitations">Limitations</a></div>
          <div><a href="./#conclusion">Conclusion</a></div>
          <div><a href="./#outlook">Outlook</a></div>
        </nav>
        <div class="toc-line"></div>
      </div>
    </d-contents>

    <div id="abstract-text">
      <h4>The Idea</h4>

      <p>
        The key idea of <i>Curriculum Induction for Safe RL</i> (CISR) <d-cite key="Turchetta2020SafeRL"></d-cite> is that a teacher trains a student to solve a given task while avoiding failure.
        This can be helpful in safety-critical systems, where failure is costly, as the agent is already being kept safe during training.
      </p>
      <p>
        To be able to save the student with a given set of interventions, the teacher needs to know how to detect dangerous states, but it does not need to know how to solve the task itself.
        The <i>curriculum policy</i> defines the order and duration in which interventions are applied.
        Learning the curriculum policy requires the teacher to train multiple students at once while assessing their performance.
      </p>

      <h4>Our Project</h4>
      <p>
        We give an interactive introduction to curriculum learning and provide the theoretical background to understand and apply the method.
        In our experiments, we compare the students trained by the Optimized curriculum policy proposed by Turchetta et al. <d-cite key="Turchetta2020SafeRL"></d-cite> to students trained with our own curriculum policies.
      </p>
    </div>


    <h2 id="background">Background</h2>

    <p>
      For the application of RL, safety can be the deciding factor in enabling or preventing the usage of a system.
      This is especially true for physical systems, as they can degrade or destroy themselves or even their environment.
      Thereby, it is not only important for the system to be safe after deployment, but also during training in the real world.
      Approaches to RL safety include Constrained Markov Decision Processes (CDMPs), as used in this work, budgeted MDPs, and Lyapunov functions.
      <d-cite key="DulacArnold2019ChallengesOR"></d-cite>
    </p>

    <p>
      An example application for the need of safety during training of RL systems are autonomous cars.
      While simulations are helpful and maybe a good starting point, training on the streets is still necessary.
      During this process, it is crucial to prevent crashes and harm to people, property and the car itself.
      <d-cite key="Turchetta2020SafeRL"></d-cite>
    </p>

    <p>
      CISR, as a form of curriculum learning, relies on a teacher as an aid during training of the agent, which is analogously called the student.
      To be able to help the student, the teacher is given a set of interventions and has to decide on when to apply which.
      For example, when teaching a child how to ride a bike, possible interventions may be adding training wheels, catching them when they fall or giving them knee and elbow guards.
      The order in which the interventions are applied has to be optimized as it can significantly impact the student's performance.
      In our analogy, it may be detrimental to skip the training wheels or to never remove them, since this would either give them no chance at getting started to learn the task or improving themselves.
      By training multiple students, the teacher is able to learn an optimal curriculum policy.
      <d-cite key="Turchetta2020SafeRL"></d-cite>
    </p>

    <p>
      In contrast to "learning from demonstration" <d-cite key="Argall2009ASO"></d-cite>, curriculum learning does not expect the teacher to know how to solve the task,
      but rather relies on the teacher to supervise and structure the learning process.
      A partly similar approach to CISR was introduced by Graves et al. <d-cite key="Graves2017AutomatedCL"></d-cite>,
      using a nonstationary multi-armed bandit algorithm to determine an optimized curriculum.
    </p>

    <p>
      CISR can also be viewed as a "meta-learning framework" <d-cite key="Vanschoren2018MetaLearningAS"></d-cite>, optimizing the curriculum policy as a hyperparameter.
      In practice, the curriculum policy could be optimized in simulation or in simplified settings before being deployed for the actual training, where training time is scarce. 
      For example, this could make the training with physical robots faster and more secure.
      <d-cite key="Turchetta2020SafeRL"></d-cite>
    </p>


    <h2 id="methodology">Methodology</h2>

    <p>
      In CISR, the student is a RL agent trained in a Constrained Markov Decision Process (CMDP), which is created by the teacher in each interaction unit using an intervention $i$ as described below.
    </p>
    <p style="text-align:center">
      $\mathcal{M}_i = \langle \mathcal{S},\mathcal{A},\mathcal{P}_i,r_i,\mathcal{D}, \mathcal{D}_i \rangle$
    </p>
    <ul>
      <li>$\mathcal{S}, \mathcal{A}$: State and action space</li>
      <li>$\mathcal{P}_i(s'|s,a)$: Transition kernel mapping pairs of states $s\in \mathcal{S}$ and actions $a\in A$ to new states $s'\in \mathcal{S}$</li>
      <li>$r_i:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$: Reward function</li>
      <li>$\mathcal{D}$: Set of unsafe terminal states</li>
      <li>$\mathcal{D}_i$: Set of trigger states</li>
    </ul>

    <p>
      The teacher gets a set $\mathcal{I}$ of interventions $\{ \langle \mathcal{D}_i, \mathcal{T}_i \rangle \}_{i=1}^K$ as input, which consist of trigger states $\mathcal{D}_i \subset \mathcal{S}$ and reset distributions $\mathcal{T}_i: \mathcal{S} \rightarrow \Delta_{\mathcal{S} \backslash \mathcal{D}_i}$.
    </p>
    <p>
      At he beginning of each interaction unit $n\in [N_s]$, the teacher decides on an intervention $i_n \in \mathcal{I}$, which induces the CMDP $M_{i_n}$ described above. This decision is done using the teachers curriculum policy $\pi^T: \mathcal{H} \rightarrow \mathcal{I}$, which maps the teachers observation history $\phi(\pi_1),...,\phi(\pi_{n-1})\in\mathcal{H}$ to the intervention $i_n$. As it is learned, we call this the optimized curriculum policy from now on.
    </p>
    <p>
      Curriculum policies independent of the student's policy (e.g. SR, HR, Back or Incremental) are simply a mapping $\pi^T:[N_s]\rightarrow \mathcal{I}$, assigning each interaction unit a specific intervention.
    </p>
    <p>
      The sequence of CMDPs $\mathcal{M}_{i_1},...,\mathcal{M}_{i_{N_s}}$, induced from the curriculum policy, is called curriculum.
    </p>
    <p>
      If the student enters a trigger state $s\in \mathcal{D}_i$, the transition is modified such that $\mathcal{P}_i(s'|s,a) = \mathcal{T}_i(s'|s)$. Additionally, the reward is cleared, i.e. $r_i(s,a,s')=0$. To prevent the student from relying on interventions, a constraint on the number of times the teachers can help the student is set. It is enforced by the CMDP solver, which penalizes the student for excessive use of the teachers help.
    </p>

    <d-figure>
      <div style="max-width: 25em; margin: 0 auto;">
        <%= require("../static/images/optimized_switch.svg") %>
      </div>
      <figcaption>
        The Optimized curriculum policy switching interventions from Soft Reset 1 (SR1 moves the agent one step back) to Hard Reset (HR resets the agent back to the start).
      </figcaption>
    </d-figure>

    <h4>Training</h4>

    <p>
      Teacher learns online in $N_t$ rounds and plays a decision rule $\pi^T_j$ that makes a new student $j$ learn under an adaptively constructed sequence of CMDPs $\mathcal{M}_{i_n}$.
      Each student $j$ learns via $N_s$ interaction units, updating its policy by transferring between units. Then, the teacher computes features $\phi$ by evaluating the student's policies, based on which it proposes the next CMDP.
      At the end of each round, the teacher adjusts its decision rule.
    </p>

    <d-figure>
      <div style="max-width: 30em; margin: 0 auto;">
        <%= require("../static/images/cisr.svg") %>
      </div>
      <figcaption>
        The CISR algorithm<d-cite key="Turchetta2020SafeRL"></d-cite>.
      </figcaption>
    </d-figure>

    <h2 id="experiments">Experiments</h2>


    <h4>Environments</h4>

    <p>
      We used the two environments which are shown below. Both are based on the Frozen Lake environment from the gym-library <d-cite key="gym-flake"></d-cite>.
      The idea of the environments is, that the agent should find its way over a frozen lake, from start to goal, while avoiding holes in the ice.
      Because ice is slippery, there is a 20% chance that the agent moves to the side, instead of forward. This also causes the game to be non-deterministic.
      The trigger states, which the teacher uses to detect when interventions should be done, are positioned around the dangerous holes.
      The environments are implemented as two-dimensional square grid-worlds, where the agent can move up/down/left/right.
    </p>

    <d-figure>
      <div style="max-width: 30em; margin: 0 auto;">
        <%= require("../static/images/maps.svg") %>
      </div>
      <figcaption>The Frozen Lake environment used in the paper<d-cite key="Turchetta2020SafeRL"></d-cite> on the left (size 10x10) and our Frozen Smiley environment on the right (size 16x16). Interventions are triggered at distance = 1 from holes.</figcaption>
    </d-figure>

    <h4>Curriculum Policies</h4>

    <h5>The Back Policy</h5>
    <p>
      One of the simplest curriculum policy one could think of involves always going back $x$ steps when a trigger state is visited.
      In our experiments, we tested values for $x$ in the interval $[1,9]$.
    </p>

    <d-code block="" language="python">
      class Back(object):
      """
      Teacher that goes back a constant number of steps
      """
      def __init__(self, action_sequence, x=None):
          self.actions = action_sequence
          self.x = x

      def predict(self, obs):
          return self.actions[self.x - 1], None
    </d-code>

    <d-figure id="back-visualization">
      <div id="back-visualization-target"></div>
      <figcaption>
        Successes for the Optimized curriculum policy and the Back policy for different values of $x$.
      </figcaption>
    </d-figure>

    <h5>The Incremental Policy</h5>

    <p>
      The idea behind the Incremental curriculum policy is the tradeoff between exploration and exploitation.
      While the agent should be free to explore the map in the beginning, it should be punished harder for failures as the learning process progresses.
      This is realized by incrementally increasing the amount of steps the agent is reset linearly.
      Formally, Incremental$_x$ resets the agent by $\lceil \frac{1}{2^x} \cdot n \rceil$ steps in the $n^{\textrm{th}}$ curriculum step.
      During our experiments, we tried out values for $x$ in the range $[0,4]$.
    </p>

    <d-code block="" language="python">
      class IncrementalTeacher(object):
      """
      Incremental heuristic teacher that increases the buffer
      size on each curriculum step
      """
      def __init__(self, action_sequence, x=None):
          self.actions = action_sequence
          self.step = 0
          self.x = x

      def predict(self, obs):
          action = int(np.ceil((1/(2 ** self.x)) * self.step))
          self.step += 1
          return self.actions[action], None
    </d-code>

    <d-figure id="incremental-visualization">
      <figure>
        <div id="incremental-visualization-target"></div>
        <figcaption>
          Successes for the Optimized curriculum policy and the Incremental policy for different values of $x$.
        </figcaption>
      </figure>
    </d-figure>


    <h2 id="results">Results</h2>

    <h4>Student Performance</h4>

    <p>
      Adding the teacher with its trigger states and intervention transitions to the maps kept the students safe during training.
      When choosing the best parameters out of those we tested, the Back and Incremental policy were able to outperform the Optimized one.
      As the Frozen Smiley environment is slightly larger than the original one, the increasing path lengths made an increase in reset steps for the Back policy necessary.
      Similarly, the Incremental curriculum policy took advantage of a longer exploration phase in the larger environment, by slowing down the increase of reset steps.
    </p>

    <figure>
      <div style="max-width: 25em; margin: 0 auto;">
        <%= require("../static/images/small_successes.svg") %>
      </div>
      <figcaption>Success rates of different curriculum policies on the Frozen Lake environment. For our policies, the best found parameters $x$ are used.</figcaption>
    </figure>

    <figure>
      <div><table style="width: 100%" id="table">
        <thead>
          <tr>
            <th></th>
            <th scope="col">Successes</th>
            <th scope="col">Training Failures</th>
            <th scope="col">Average Returns</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <th scope="row">Back$_6$</th>
            <td class="no">$0.921$</td>
            <td>$0.000$</td>
            <td class="no">$5.259$</td>
          </tr>
          <tr>
            <th scope="row">Incremental$_2$</th>
            <td>$0.886$</td>
            <td>$0.000$</td>
            <td>$5.053$</td>
          </tr>
          <tr>
            <th scope="row">Trained</th>
            <td>$0.743$</td>
            <td>$0.000$</td>
            <td>$4.035$</td>
          </tr>
          <tr>
            <th scope="row">Original</th>
            <td>$0.641$</td>
            <td class="no">$3672.600$</td>
            <td>$3.586$</td>
          </tr>
          <tr>
            <th scope="row">HR</th>
            <td>$0.000$</td>
            <td>$0.000$</td>
            <td>$-0.295$</td>
          </tr>
          <tr>
            <th scope="row">SR1</th>
            <td>$0.820$</td>
            <td>$0.000$</td>
            <td>$4.661$</td>
          </tr>
          <tr>
            <th scope="row">Bandit</th>
            <td>$0.659$</td>
            <td>$0.000$</td>
            <td>$3.638$</td>
          </tr>
        </tbody>
      </table></div>
      <figcaption>
        Success rates, training failures and average returns of different curriculum policies on the Frozen Lake environment.
        All teachers keep the student safe while training the student in the original environment leads to failures during training.
        On average, the Back and Incremental policy outperform the others. 
      </figcaption>
    </figure>

    <figure>
      <div style="max-width: 25em; margin: 0 auto;">
        <%= require("../static/images/16x16_successes.svg") %>
      </div>
      <figcaption>
        Success rates of different curriculum policies on the Frozen Smiley environment. For our policies, the best found parameters $x$ are used.
      </figcaption>
    </figure>

    <figure>
      <div><table style="width: 100%" id="table">
        <thead>
          <tr>
            <th></th>
            <th scope="col">Successes</th>
            <th scope="col">Training Failures</th>
            <th scope="col">Average Returns</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <th scope="row">Back$_8$</th>
            <td class="no">$0.929$</td>
            <td>$0.000$</td>
            <td class="no">$5.293$</td>
          </tr>
          <tr>
            <th scope="row">Incremental$_4$</th>
            <td>$0.886$</td>
            <td>$0.000$</td>
            <td>$5.054$</td>
          </tr>
          <tr>
            <th scope="row">Trained</th>
            <td>$0.879$</td>
            <td>$0.000$</td>
            <td>$4.856$</td>
          </tr>
          <tr>
            <th scope="row">Original</th>
            <td>$0.741$</td>
            <td class="no">$2453.400$</td>
            <td>$4.166$</td>
          </tr>
          <tr>
            <th scope="row">HR</th>
            <td>$0.000$</td>
            <td>$0.000$</td>
            <td>$-0.140$</td>
          </tr>
          <tr>
            <th scope="row">SR1</th>
            <td>$0.597$</td>
            <td>$0.000$</td>
            <td>$3.367$</td>
          </tr>
          <tr>
            <th scope="row">Bandit</th>
            <td>$0.398$</td>
            <td>$0.000$</td>
            <td>$2.133$</td>
          </tr>
        </tbody>
      </table></div>
      <figcaption>
        Success rates, training failures and average returns of different curriculum policies on the Frozen Smiley environment.
        All teachers keep the student safe while training the student in the original environment leads to failures during training.
        On average, the Back and Incremental policy outperform the others. 
      </figcaption>
    </figure>


    <h4>Trajectories</h4>

    <p>
      The figure below shows a visualization of the paths taken by the students during training.
      During the first curriculum steps we can see the students exploring the map, while at later steps, they follow the previously found path, with only slight deviations or optimizations.
    </p>

    <d-figure id="trajectories-visualization">
      <figure>
        <br/>
        <div id="trajectories-visualization-target"></div>
        <figcaption>
          Exemplary trajectories for the Frozen Smiley environment with the Optimized policy. 
          The lines represent the steps taken, while the background shows a heatmap of the student's positions.
          The trajectories show a progression from the first curriculum step to a later step.
          <!--TODO: maybe use different trajectories and optimize images-->
        </figcaption>
      </figure>
    </d-figure>


    <h2 id="limitations">Limitations</h2>

    <p>
      While the described method has several clear advantages, there are also disadvantages and limitations coming along with it.
      The most obvious problem is the difficulty of defining suitable trigger states for the teacher.
      But also a set of possible interventions has to defined, and the teacher needs to be trained too, requiring additional computational resources.
    </p>

    <p>
      Some tasks might also be simply so risky, that the teacher prevents the student from solving them at all, because it prevents the student
      from taking necessary risks.
      An example for this can be the game at the beginning, if the "Frozen Smiley" environment and the "Distance 2" teacher are selected.
      In these cases, less restrictive trigger states are needed, which might have to allow failures during training.
    </p>

    <!--TODO: Write about applicability difficulties for other environments (e.g. racing car or pole stick)-->


    <h2 id="conclusion">Conclusion</h2>
    
    <p>
      During our experiments, we found that it is possible to define simple curriculum policies, which outperform a learned policy with little effort.
      We also observed that larger environments require a longer exploration phase and that the original HR, SR and Bandit policies do not generalize well to larger environments.
      Fundamentally, we found that defining reset transitions which keep the student safe is easier than defining suitable trigger states in the first place.
      When state spaces become more complex, dynamic or just partly observable, this can become a problem.
    </p>


    <h2 id="outlook">Outlook</h2>

    <p>
      Going forward, the method could be applied to OpenAI's Safety Gym environments to test the robustness of different strategies.
      Additionally, the amount of available interventions for the Optimized curriculum policy could be increased to account for this complexity.
      Finally, it has to be evaluated how well our curriculum policies generalize to dynamic or random environments.
    </p>

  </d-article>


  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      We are grateful to Rong Guo for supervising this project and continuously giving us feedback.
    </p>

    <h3>Author Contributions</h3>
    <p>
      This article was co-authored by Marvin Sextro and Jonas Loos under supervision of Rong Guo. Klaus Obermayer provided feedback on the project.
    </p>
    <p>* equal contributions</p>

    <h3>Additional Material</h3>
    <p>
      For a summary of the main findings presented in this article, also see our <a href="https://raw.githubusercontent.com/Safe-RL-Team/curriculum-learning-poster/main/curriculum_learning_poster.pdf">conference poster</a>.
    </p>

    <h3>Updates and Corrections</h3>
    <p>
      If you see mistakes or want to suggest changes, please <a href="https://github.com/Safe-RL-Team/curriculum-learning/issues/new">create an issue on GitHub</a>.
    </p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>